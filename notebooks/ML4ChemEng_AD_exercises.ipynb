{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpGuLGUEcQsW"
      },
      "outputs": [],
      "source": [
        "# Authors:\n",
        "\n",
        "# Optimisation and Machine Learning for Process Systems Engineering:\n",
        "# https://www.imperial.ac.uk/optimisation-and-machine-learning-for-process-engineering/about-us/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15FydNGVdY4A"
      },
      "source": [
        "* Anomalous data is scarce, therefore in this type of problems we can only use normal operation data for training. The few instances of fault behavior we have are left for cross-validation purposes.\n",
        "* We face a semisupervised problem. We know the labels of normal instances only and we only feed the training with them. The distinction between both classes must be made *after* and not *during* training through some kind of threshold tuning.\n",
        "* Typical methods used range from statistical, neural networks, distance-based, and other unsupervised and semi-supervised methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l92at3rAqU"
      },
      "source": [
        "# Warm-up: Probabilistic approach with a single variable example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu6vni0jrKAw"
      },
      "source": [
        "In this example, we are generating temperature data from a reactor. We have two sets of data:\n",
        "\n",
        "- The training set, which we are certain contains normal operation conditions (NOC) only\n",
        "- The cross-validation set, which contains a mix of both normal and anomaly conditions\n",
        "\n",
        "We will use statistical methods to try to predict the state of the system in the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "np.random.seed(52)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization\n",
        "mean = 400\n",
        "std = 3\n",
        "n_noc_test = 100\n",
        "n_fault_slice=50\n",
        "n_slice=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "6W5Tl10Ntve1",
        "outputId": "fa7efb17-85ac-48f7-8b3f-eb594433a571"
      },
      "outputs": [],
      "source": [
        "def gen_data_train(mean, std, n_noc=500):\n",
        "    # Generate normal operation conditions (NOC) data\n",
        "    X_train = np.random.normal(mean, std, n_noc)    \n",
        "    return X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_data_test(mean, std, n_noc=100, n_fault_slice=50, n_slice=10):\n",
        "    # Generate anomaly data with drift. 50 slices of 10 points each\n",
        "    changeRate = 1.02\n",
        "    X_test = np.random.normal(mean, std, n_noc)\n",
        "\n",
        "    poisson_variable = np.random.poisson(0.05, n_fault_slice)\n",
        "    mu_fault = mean\n",
        "    sigma_fault = std\n",
        "    for i in range(0, n_fault_slice):\n",
        "        mu_fault = mu_fault * (changeRate ** poisson_variable[i])\n",
        "        sigma_fault = sigma_fault * (changeRate ** poisson_variable[i])\n",
        "        next_fault_data = np.random.normal(mu_fault, sigma_fault, n_slice)\n",
        "        X_test = np.concatenate((X_test, next_fault_data)) \n",
        "          \n",
        "    return X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_monovariate_data(mean, std, n_noc_test, n_fault_slice, n_slice):\n",
        "    X_train = gen_data_train(mean, std)\n",
        "    X_test = gen_data_test(mean, std, n_noc_test, n_fault_slice, n_slice)\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test = gen_monovariate_data(mean, std, n_noc_test, n_fault_slice, \n",
        "                                       n_slice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We should check the **normality assumption** (assume that our data follows a Gaussian distribution) of the generated data. A typical way is to plot a quartile-quartile plot and check its linearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import pylab\n",
        "\n",
        "sm.qqplot((X_train - X_train.mean(axis=0)) / X_train.std(axis=0), line='45')\n",
        "pylab.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-series plot\n",
        "x = np.linspace(X_train.min() - 2, X_train.max() + 2, 1000)\n",
        "plt.plot(X_train)\n",
        "plt.title('Normal operation data against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "plt.hist(X_train, 20, density=True, facecolor='g', alpha=0.75, label='data')\n",
        "plt.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "plt.xlabel('Temperature')\n",
        "plt.xlim(min(x), max(x))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwELJeb620IA"
      },
      "source": [
        "We can select a specific threshold over which any value will be considered an anomaly. A typical one is $2\\sigma$, which corresponds with about a 95 % confidence level\n",
        "\n",
        "Note that, in real-life examples, we do not know the exact mean and standard deviation of our data. Therefore, the way to work with them is estimating the mean and standard deviation of our sample in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "DZapnodm2mwh",
        "outputId": "f8395b59-4cd6-4da4-972b-e45a7731e5d9"
      },
      "outputs": [],
      "source": [
        "mean_train = X_train.mean()\n",
        "std_est = X_train.std()\n",
        "th_lower = mean_train - 2 * std_est\n",
        "th_higher = mean_train + 2 * std_est\n",
        "print('Mean: ', mean_train)\n",
        "print('Standard deviation: ', std_est)\n",
        "print('Thresholds: ', th_lower, th_higher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize how these thresholds look like in the probability density function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Gvg37cyh2x9p",
        "outputId": "584dacd7-807b-4472-fbd9-780da2fc0406"
      },
      "outputs": [],
      "source": [
        "# Get NOC bounds\n",
        "x_fill = np.linspace(th_lower, th_higher)\n",
        "fig_pdf, ax_pdf = plt.subplots()\n",
        "ax_pdf.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "ax_pdf.fill_between(x_fill, scipy.stats.norm.pdf(x_fill, mean, std), color='grey')\n",
        "# Get anomaly bounds\n",
        "x_fault = np.linspace(min(x), th_lower)\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "x_fault = np.linspace(th_higher, max(x))\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "ax_pdf.set_xlim(mean - 10, mean + 10)\n",
        "ax_pdf.set_ylim([0, None])\n",
        "ax_pdf.set_xlabel('Temperature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series plot\n",
        "x_fault = np.arange(1, len(X_test) + 1)\n",
        "plt.plot(x_fault, X_test)\n",
        "plt.axvline(n_noc_test, color='salmon', label='fault starts')\n",
        "\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_higher, th_higher], 'r--')\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_lower, th_lower], 'r--')\n",
        "\n",
        "plt.title('Anomaly generation against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "ax_pdf.hist(X_test, 20, density=True, facecolor='g', alpha=0.75, label='anomaly data')\n",
        "ax_pdf.legend(loc='best')\n",
        "fig_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance on cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4wyYVbJgT4p"
      },
      "source": [
        "Let's obtain a metric for how well does this anomaly detector worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "F-dyQgbsgToV",
        "outputId": "a3f709b8-512a-4143-e964-85a92fd0e44f"
      },
      "outputs": [],
      "source": [
        "# First we get the actual labels of our cross-validation set\n",
        "# We know that the first 100 instances are normal, while the fault starts\n",
        "#  from that point onwards\n",
        "y_true = np.concatenate((\n",
        "    np.zeros((n_noc_test,)),\n",
        "    np.ones((n_fault_slice * n_slice))\n",
        "))\n",
        "y_pred = (X_test > th_higher) | (X_test < th_lower)\n",
        "\n",
        "# Plot our prediction\n",
        "plt.plot(x_fault[y_pred == 0], X_test[y_pred == 0], 'b.', label='predicted as normal')\n",
        "plt.plot(x_fault[y_pred == 1], X_test[y_pred == 1], 'rx', label='predicted as anomaly')\n",
        "plt.title('Anomalies detected over time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Use the accuracy_score function from skit-learn\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"The obtained accuracy is {accuracy * 100:0.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnOde5Wjut3"
      },
      "source": [
        "Note that due to our threshold, a few false positives take place. This is a common trade-off in anomaly detection:\n",
        "- **It is usually difficult to avoid both false positives and false negatives** at the same time\n",
        "- The determination of the threshold is an important task that the engineer must take into account to give more weight to one or another.\n",
        "- This depends heavily on the application: e.g. in medical applications, false positives are highly discouraged, while spam detection might be more indulgent in that sense\n",
        "\n",
        "Also note that the fault takes quite a while to manifest, from it start at $t=100$ min to the first visually noticeable drift at aroun $t=300$ min. More advanced methods might help us anticipate this anomaly but, without more variables showing more hints, it can be impossible to increase our accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sJovavKqG0U"
      },
      "source": [
        "# Probabilistic approach - Multivariate example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGsiNIJ6qMf4"
      },
      "source": [
        "Now, consider a more complex case with generic 10 variables that conform a dataset representative of the state of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncIVzRVDwDCa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IwHfD1AqFzN"
      },
      "outputs": [],
      "source": [
        "def generate_multivariate_normal_dataset(mean, covariance, n, perc_noc, changing_rate, rng=None, shuffle=False):\n",
        "  n_noc = int (n * perc_noc)\n",
        "  n_fault = n - n_noc\n",
        "  x_noc = np.zeros((n_noc, 10))\n",
        "  x_fault = np.zeros((n_fault, 10))\n",
        "  cols = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label']\n",
        "\n",
        "  # Generate data under normal conditions\n",
        "  x_noc = rng.multivariate_normal(mean=mean, cov=covariance, size=n_noc)\n",
        "  label_noc = np.zeros(n_noc)\n",
        "  label_noc = label_noc.reshape((len(label_noc), 1))\n",
        "  data_noc = np.concatenate((x_noc, label_noc), axis=1)\n",
        "\n",
        "  # Generate data include fault\n",
        "  poisson_generator = rng.poisson(0.2, n_fault)\n",
        "  label_poisson = np.ones(n_fault)\n",
        "  for j in range(n_fault):\n",
        "    if poisson_generator[j] != 0:\n",
        "        label_poisson[0:j+1] = 0\n",
        "        break\n",
        "\n",
        "  # Depends on whether mean or cov is changing\n",
        "  mean_fault = mean\n",
        "  cov_fault = covariance\n",
        "\n",
        "  for i in range(x_fault.shape[0]):\n",
        "    if poisson_generator[i] != 0:\n",
        "        cov_fault[0,1] *= (1+changing_rate)**(poisson_generator[i])\n",
        "        cov_fault[1,0] *= (1+changing_rate)**(poisson_generator[i])\n",
        "    instance = rng.multivariate_normal(mean=mean_fault, cov=cov_fault, size=1)\n",
        "    x_fault[i] = instance\n",
        "\n",
        "  label_poisson = label_poisson.reshape((len(label_poisson), 1))\n",
        "  data_fault = np.concatenate((x_fault, label_poisson), axis=1)\n",
        "\n",
        "  data_np = np.concatenate((data_noc, data_fault), axis=0)\n",
        "  data = pd.DataFrame(data_np, columns=cols)\n",
        "\n",
        "  if shuffle == True:\n",
        "    data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return data, data_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CzmK7cowFfb",
        "outputId": "68e77837-f700-42c5-f3fc-bc660c5d5fda"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "n_samples = 500\n",
        "perc_noc_train = 1   # % NOC instances in the training set (100 %)\n",
        "perc_noc_test = 0.2  # % NOC instances in the test set (100 %)\n",
        "changing_rate = 0.15\n",
        "base_rng      = np.random.default_rng(42)\n",
        "\n",
        "# Generate means\n",
        "mean = base_rng.integers(low=1, high=10, size=10)\n",
        "print(mean)\n",
        "\n",
        "# Generate covariance\n",
        "covariance = np.array([[2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0]])\n",
        "print(covariance)\n",
        "\n",
        "# Generate numpy data and do train-test split\n",
        "data_train, data_train_np = generate_multivariate_normal_dataset(\n",
        "    mean, covariance, n_samples, perc_noc_train, changing_rate, rng=base_rng, \n",
        "    shuffle=False)\n",
        "data_test, data_test_np = generate_multivariate_normal_dataset(\n",
        "    mean, covariance, n_samples, perc_noc_test, changing_rate, rng=base_rng, \n",
        "    shuffle=False)\n",
        "X_train = data_train.drop(['label'], axis=1).to_numpy()\n",
        "y_train = data_train['label'].to_numpy()\n",
        "X_test = data_test.drop(['label'], axis=1).to_numpy()\n",
        "y_test = data_test['label'].to_numpy()\n",
        "\n",
        "# The features of the dataset correspond to the columns of X_train\n",
        "n_features = X_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBLnEGXzUQm"
      },
      "source": [
        "In this case, the dataset is completly random, and disturbances are introduced in the covariance between the first and second variable, which starts suffering a drift based on a Poisson disturbance like in the single-variable example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "LS4t04AdyNMT",
        "outputId": "41d12fb9-7acc-49bb-ef3d-1ab01f2327b0"
      },
      "outputs": [],
      "source": [
        "plt.plot(data_test.iloc[:300,0], label='test')\n",
        "plt.plot(data_train.iloc[:300,0], label='train')\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anomaly detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to estimate the detection thresholds for each variable separately. For this, we use the probability density function of the normal distribution\n",
        "\n",
        "$f(x_i|\\mu, \\Sigma) = \\frac{1}{{\\sqrt{(2 \\pi)^n \\Sigma}}} e^{-\\frac{1}{2}{(x - \\mu)^T \\Sigma^{-1} {(x - \\mu)}}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the probability density function is not exactly yielding probabilities, but a measure of the density of the random distribution at a specific point. However, it can be used to locate points in the probability distrubution ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def probability_density(X, mean, var):\n",
        "    \"\"\"\n",
        "    Calculate the probability density of a series of instances in X given the\n",
        "    multivariate mean and covariance matrix.\n",
        "    \"\"\"\n",
        "    n = len(mean)\n",
        "    if var.ndim == 1:\n",
        "        var = np.diag(var)\n",
        "    X = X - mean\n",
        "    p = (2 * np.pi)**(- n/2) * np.linalg.det(var)**(-0.5) * \\\n",
        "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
        "    if len(p) == 1:\n",
        "        return p[0]\n",
        "    else:\n",
        "        return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we estimate the means and covariance of our system using the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trGRozQC5nBD"
      },
      "outputs": [],
      "source": [
        "# Estimate Gaussian distribution\n",
        "mean_train = np.mean(X_train, axis=0)\n",
        "cov_train = np.cov(X_train, rowvar=False)  # By default it deals with each row as a variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the threshold using the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_threshold(scores, y_true, target_acc=0.95, positive_lower=True, \n",
        "                  n_samples=1e4, print_stdout=True, plot=True):\n",
        "    \"\"\" \n",
        "    Obtain threshold by searching for the closest accuracy to target_acc via \n",
        "    brute force.\n",
        "    ----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    positive_lower: if True, poisitive instances are predicted when the score\n",
        "        is lower than the threshold. If False, positive instances are predicted\n",
        "        for socres higher than the threshold\n",
        "    \"\"\"\n",
        "    # Get extremes of scores\n",
        "    scores_range = np.linspace(scores.min(), scores.max(), int(n_samples))\n",
        "    # Loop them until finding the closer value to 95 % accuracy (brute force)\n",
        "    error_old = 1\n",
        "    accs = {}\n",
        "    for score_th in scores_range:\n",
        "        # Obtain predictions given the current threshold\n",
        "        if positive_lower:\n",
        "            y_pred = scores < score_th\n",
        "        else:\n",
        "            y_pred = scores > score_th\n",
        "        # Calculate accuracy and measure the error with respect target_acc\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        accs[score_th] = acc\n",
        "        error = (target_acc - acc) ** 2\n",
        "        if print_stdout:\n",
        "            print(f\"Obtained accuracy {acc}, error {error}\" \n",
        "                  f\" with threshold = {score_th}\")\n",
        "        if error < error_old:\n",
        "            threshold = score_th \n",
        "            error_old = error\n",
        "        elif error > error_old:\n",
        "            # Avoid continuing since no better error will be obtained\n",
        "            break\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "        plt.plot(pd.Series(accs), label='evolution of accuracy')\n",
        "        plt.vlines(threshold, plt.gca().get_ylim()[0], 1, color='red', label='final threshold')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(loc='best')\n",
        "    return threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probabilities and threshold\n",
        "probs = probability_density(X_train, mean_train, cov_train)\n",
        "threshold = get_threshold(probs, y_train, positive_lower=True, n_samples=1e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check we obtain the desired accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = probability_density(X_train, mean_train, cov_train) < threshold\n",
        "print(f'We should have {int(len(X_train) * 0.05)} false positives.')\n",
        "print(f'We have {sum(y_train_pred)} false positives')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our threshold, we can evaluate the test set. Remember we use the fitted mean and variance from the training set, we do not obtain these from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_test = probability_density(X_test, mean_train, cov_train) \n",
        "y_test_pred = p_test < threshold\n",
        "acc_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'The obtained accuracy is {acc_test *100} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proximity-based approaches - Clustering example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we are going to work with the iris dataset, which has become famous among the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Iris dataset](https://upload.wikimedia.org/wikipedia/commons/c/cb/Flores_de_%C3%8Dris.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise**: Use k-means clustering to perform anomaly detection on the iris dataset.\n",
        "\n",
        "- The training set consists on the 'setosa' and 'virginica' (first and third class) data points.\n",
        "- The test set consists on the 'versicolor' instances.\n",
        "- As a simplification, use only the first and the last features of the dataset: the sepal length and the petal width.\n",
        "\n",
        "New instances should be classified as an anomaly if they do not belong to one of the known labels: 'setosa' or 'virginica'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "iris.feature_names, iris.target_names\n",
        "\n",
        "n_clusters = 2  # We assume our training set is composed of 2 clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "fig_orig, ax_orig = plt.subplots()\n",
        "scatter = ax_orig.scatter(iris.data[:, 0], iris.data[:, 3], c=iris.target)\n",
        "ax_orig.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax_orig.legend(scatter.legend_elements()[0], iris.target_names, \n",
        "          loc=\"lower right\", title=\"Classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform train-test split\n",
        "y = iris.target\n",
        "X_train = iris.data[:, [0, 3]][y != 1]  # [0,3] to get first and fourth feature\n",
        "                                        # [y != 1] to get all classes except 1\n",
        "X_test = iris.data[:, [0, 3]][y == 1]   # Same for the test set\n",
        "y_train = np.zeros((len(X_train),))\n",
        "y_test = np.ones((len(X_test),))\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit k-means model with two clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(X_train)\n",
        "kmeans.labels_, kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot cluster predictions\n",
        "_, ax = plt.subplots()\n",
        "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=kmeans.labels_)\n",
        "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax.legend(scatter.legend_elements()[0], iris.target_names, \n",
        "          loc=\"lower right\", title=\"Classes\")\n",
        "print('We can see that k-means correctly separates the two classes except for'\n",
        "      'one single point')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have fitted our model we can try to predict anomalies. The algorithm would go as follows. For any new instance:\n",
        "\n",
        "1. Associate the new instance with the closest cluster centroid.\n",
        "2. Calculate a probabilistic/distance-based anomaly metric\n",
        "3. Classify the new instance as normal or anomaly depending on a threshold previously selected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We calculate a threshold for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[kmeans.labels_ == 1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "th_clusters = []\n",
        "mean_clusters = []\n",
        "cov_clusters = []\n",
        "for i in range(n_clusters):\n",
        "    X_train_cluster = X_train[kmeans.labels_ == i]\n",
        "    y_train_cluster = np.zeros(len(X_train_cluster))\n",
        "    # Estimate Gaussian distribution of each cluster\n",
        "    mean_train = np.mean(X_train_cluster, axis=0)\n",
        "    mean_clusters.append(mean_train)\n",
        "    cov_train = np.cov(X_train_cluster, rowvar=False)\n",
        "    cov_clusters.append(cov_train)\n",
        "    # Get probabilities and threshold\n",
        "    probs = probability_density(X_train_cluster, mean_train, cov_train)\n",
        "    th = get_threshold(probs, y_train_cluster, positive_lower=True, \n",
        "                       n_samples=1e4, print_stdout=False, plot=False)\n",
        "    th_clusters.append(th)\n",
        "print('Thresholds are', th_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the accuracy on our training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = np.zeros(len(X_train))\n",
        "scores = np.zeros(len(X_train))\n",
        "for i in range(len(X_train)):\n",
        "    # Rehape instance to make it 2-dimensional\n",
        "    x = X_train[i, :].reshape(1, -1)\n",
        "    cluster_idx = kmeans.labels_[i]\n",
        "    # print(f'Point {i} belongs to cluster {cluster_idx}')\n",
        "    # Calculate the Mahalanobis distance\n",
        "    scores[i] = probability_density(\n",
        "        x, mean_clusters[cluster_idx], cov_clusters[cluster_idx])\n",
        "    # Predict anomaly\n",
        "    y_train_pred[i] = scores[i] < th_clusters[cluster_idx]\n",
        "print(f'The accuracy on the train set is {accuracy_score(y_train, y_train_pred) * 100} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred)\n",
        "ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[3])\n",
        "ax.legend(scatter.legend_elements()[0], ['normal', 'anomaly'], \n",
        "          loc=\"lower right\", title=\"Classes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For new instances (test set) determine closest cluster and compare distances to threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred = np.zeros(len(X_test))\n",
        "scores = np.zeros(len(X_test))\n",
        "for i in range(len(X_test)):\n",
        "    # Rehape instance to make it 2-dimensional\n",
        "    x = X_test[i, :].reshape(1, -1)\n",
        "    cluster_idx = kmeans.predict(x)[0]\n",
        "    # Calculate the Mahalanobis distance\n",
        "    scores[i] = probability_density(\n",
        "        x, mean_clusters[cluster_idx], cov_clusters[cluster_idx])\n",
        "    # Predict anomaly\n",
        "    y_test_pred[i] = scores[i] < th_clusters[cluster_idx]\n",
        "print(f'The accuracy on the test set is {accuracy_score(y_test, y_test_pred) * 100} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare our anomaly detection results with the actual classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred)\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with the original classes\n",
        "fig_orig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural networks: autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks are usually trained in a supervised manner. To be able to use them in a semisupervised problem, we cannot use the typical feedforward neural network. For this, the self-associative neural networks, also called autoencoders, come to help. Their name is self-explanatory: the goal of autoencorders is to try to reproduce the input in the output of the neural network. This may look like a trivial task but the trick here is that there is a bottleneck in the architecture of the network that causes a loss of information.\n",
        "\n",
        "The training phase tries to make the reconstruction with the lowest possible loss despite the bottleneck. This way it is able to find the most important features and discard noise that is useless to reconstruct the input. Its usefulness in anomaly detection comes with that, if trained with normal operation data and learns to identify and reconstruct it correctly when an abnormal event occurs, the reconstruction will show an important bias compared with nominal operation data.\n",
        "\n",
        "Therefore, they do not use labels to train the network. However, we need to know that all the instances fed during training belong to the same class (usually the *normal operation* class), therefore the *semisupervised* surname of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise**: We will work with the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. For this, we will try to identify as an anomaly every number that is not a \"5\"\n",
        "\n",
        "![MNIST dataset](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will load the dataset from the scikit-learn builtin datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Import tensorflow to download higher resolution mnist dataset\n",
        "# Otherwise sklearn will be used\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mnist(X, y, n=3, title=''):\n",
        "    \"\"\" Distribute plots in rows of 4. \"\"\"\n",
        "    import math\n",
        "    # Get number of pixels in each dimension\n",
        "    side_px = int(math.sqrt(X.shape[1]))\n",
        "    nrows = math.ceil(n / 4)\n",
        "    ncols = 4\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "    i = 0\n",
        "    for image, label in zip(X, y):\n",
        "        #np.unravel_index(i, axes.shape)\n",
        "        ax = axes.flatten()[i]\n",
        "        ax.set_axis_off()\n",
        "        image = image.reshape(side_px, side_px)\n",
        "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "        if not label:\n",
        "            ax.set_title(f\"Label: {bool(label)}\", fontweight=\"bold\")\n",
        "        else:\n",
        "            ax.set_title(f\"Label label: {bool(label)}\")\n",
        "        i += 1\n",
        "        if i >= n:\n",
        "            break\n",
        "    fig.suptitle(title)\n",
        "\n",
        "\n",
        "def get_mnist_5(split=False):\n",
        "    # Load data\n",
        "    try:\n",
        "        (X_train, y_train), (X_test, y_test) = \\\n",
        "            tf.keras.datasets.mnist.load_data()\n",
        "        # Unroll pixels to follow sklearn setup\n",
        "        X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "        X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "        # Set up labels to this specific case\n",
        "        y_train = ~(y_train == 5)\n",
        "        y_test = ~(y_test == 5)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    except:\n",
        "        digits = datasets.load_digits()\n",
        "\n",
        "        # Get input and output data\n",
        "        X = digits.data\n",
        "        labels = digits.target\n",
        "        y = ~(labels == 5)\n",
        "\n",
        "        if split:\n",
        "            # Split data into 50% train and 50% test subsets\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, shuffle=True\n",
        "            )\n",
        "            return X_train, X_test, y_train, y_test\n",
        "        else:\n",
        "            return X, y\n",
        "    \n",
        "    \n",
        "X_train, X_test, y_train, y_test = get_mnist_5(split=True)\n",
        "plot_mnist(X_train, y_train, n=12)\n",
        "print(\"Five's are represented as False (normal), \"\n",
        "      \"while the rest are considered True (anomaly)\")\n",
        "\n",
        "# Keep only 5's for the training set\n",
        "X_train = X_train[y_train == False]\n",
        "y_train = y_train[y_train == False]\n",
        "plot_mnist(X_train, y_train, n=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each instance of the training set consists of 64 elements that represent the 8-by-8 greyscale pixel image of a handwritten number. Each pixel value ranges from 0 to 16, from white to black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('This is an example of the contents of an image.'\n",
        "      ' The zero-valued pixels correspond to white background')\n",
        "print(X_train[1, :], y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before working with the data, we need to standardize the pixel intensisty values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling (no need of centering in images)\n",
        "scale_factor = X_train.max()\n",
        "X_train_scaled = X_train / scale_factor\n",
        "X_test_scaled = X_test / scale_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have our dataset, we can set up our autoencoder neural network.\n",
        "\n",
        "First, set up the architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shape of input and latent variable\n",
        "n_input = X_train.shape[1]\n",
        "\n",
        "# Encoder structure\n",
        "n_encoder1 = 10\n",
        "# n_encoder2 = 10\n",
        "n_latent = 2\n",
        "\n",
        "# Decoder structure\n",
        "# n_decoder2 = 10\n",
        "n_decoder1 = 10\n",
        "\n",
        "hidden_layer_sizes = (n_encoder1, n_latent, n_decoder1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model using sklearn's [`MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) (Multilayer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MLPRegressor(\n",
        "    hidden_layer_sizes=hidden_layer_sizes, \n",
        "    activation='tanh', \n",
        "    solver='adam', \n",
        "    batch_size='auto', \n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=None,\n",
        "    tol=0.0000001,\n",
        "    verbose=True,\n",
        "    alpha=0.0001\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can fit the model. Bear in mind that we want the model to try to obtain a target output as similar as possible to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train_scaled, X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(model.loss_curve_)\n",
        "plt.xlabel('Training epochs')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check how reconstructions go for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_inout_mnist(image_in, image_out):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_in, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[0].set_title(\"Input\", fontsize=26)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(image_out, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[1].set_title(\"Output\", fontsize=26)\n",
        "    axes[1].axis('off')\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5\n",
        "side_px = int(np.sqrt(X_train.shape[1]))\n",
        "input_image = X_train_scaled[0, :].reshape((side_px,  side_px))\n",
        "# Perform reconstruction through autoencoder\n",
        "reconstructed_image = model.predict(X_train_scaled[0:0 + 1, :])\n",
        "side_px = int(np.sqrt(X_train.shape[1]))\n",
        "reconstructed_image = reconstructed_image.reshape((side_px, side_px))\n",
        "compare_inout_mnist(input_image, reconstructed_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot other numbers in the test set\n",
        "reconstructed_images = []\n",
        "for i in range(1, 5):\n",
        "    input_image = X_test_scaled[i, :].reshape((side_px,  side_px))\n",
        "    # Perform reconstruction through autoencoder\n",
        "    reconstructed_image = model.predict(X_test_scaled[i:i + 1, :])\n",
        "    reconstructed_image = reconstructed_image.reshape((side_px, side_px))\n",
        "    compare_inout_mnist(input_image, reconstructed_image)\n",
        "    reconstructed_images.append(reconstructed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The anomaly detection algorithm to detect new instances as normal or outliers and obtain a metric of performance is similar to previous exercises and left for the student to complete."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
