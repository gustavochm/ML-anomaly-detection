{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpGuLGUEcQsW"
      },
      "outputs": [],
      "source": [
        "# Authors:\n",
        "\n",
        "# Optimisation and Machine Learning for Process Systems Engineering:\n",
        "# https://www.imperial.ac.uk/optimisation-and-machine-learning-for-process-engineering/about-us/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMCFuQ7BcxkF"
      },
      "source": [
        "The first part of this notebook is a recap from the course. There is a practical section after this recap.\n",
        "\n",
        "You will find <font color='blue'>text in blue for a few simple planned **coding tasks** that you can implement.</font>\n",
        "\n",
        "Within the code cell, we suggest your implementation to be between the line:\n",
        "\n",
        "`============= Start of your code =============`\n",
        "\n",
        "and the line:\n",
        "\n",
        "`============= End of your code =============` **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hehqnXN9dBk4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15FydNGVdY4A"
      },
      "source": [
        "# Problem description\n",
        "\n",
        "Summary of the general anomaly detection problem\n",
        "\n",
        "* Anomaly data is scarce, therefore it is considered that we can only use normal operation data for training. The few instances of fault behavior we have are left for validation purposes\n",
        "* Therefore, we face a semisupervised problem. We know the labels of normal instances only and we only feed the training with them. The distinction between both classes must be made *after* and not *during* training through some kind of threshold tuning.\n",
        "* Typical methods used range from statistical, neural networks, distance-based (<font color='red'>summarize here more from time series analysis</font>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWz-YpREdeTh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1l92at3rAqU"
      },
      "source": [
        "# Warm-up: single variable example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu6vni0jrKAw"
      },
      "source": [
        "In this example, we are generating temperature data from a reactor. We have two sets of data:\n",
        "\n",
        "- The training set, which we are certain contains normal operation conditions (NOC) only\n",
        "- The cross-validation set, which contains a mix of both normal and anomaly conditions\n",
        "\n",
        "We will use statistical methods to try to predict the state of the system in the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "6W5Tl10Ntve1",
        "outputId": "fa7efb17-85ac-48f7-8b3f-eb594433a571"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "np.random.seed(41)\n",
        "\n",
        "# Generate normal operation conditions (NOC) data\n",
        "mean    = 400\n",
        "std = 3\n",
        "n_noc = 500\n",
        "\n",
        "X_train = np.random.normal(mean, std, n_noc)\n",
        "\n",
        "# Time-series plot\n",
        "x = np.linspace(X_train.min() - 2, X_train.max() + 2, 1000)\n",
        "plt.plot(X_train)\n",
        "plt.title('Normal operation data against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "plt.hist(X_train, 20, density=True, facecolor='g', alpha=0.75, label='data')\n",
        "plt.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "plt.xlabel('Temperature')\n",
        "plt.xlim(min(x), max(x))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwELJeb620IA"
      },
      "source": [
        "We can select a specific threshold over which any value will be considered an anomaly. A typical one is $2\\sigma$, which corresponds with about a 95 % confidence level\n",
        "\n",
        "Note that, in real-life examples, we do not know the exact mean and standard deviation of our data. Therefore, the way to work with them is estimating the mean and standard deviation of our sample in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "DZapnodm2mwh",
        "outputId": "f8395b59-4cd6-4da4-972b-e45a7731e5d9"
      },
      "outputs": [],
      "source": [
        "mean_train = X_train.mean()\n",
        "std_est = X_train.std()\n",
        "th_lower = mean_train - 2 * std_est\n",
        "th_higher = mean_train + 2 * std_est"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "Gvg37cyh2x9p",
        "outputId": "584dacd7-807b-4472-fbd9-780da2fc0406"
      },
      "outputs": [],
      "source": [
        "# Get NOC bounds\n",
        "x_fill = np.linspace(th_lower, th_higher)\n",
        "fig_pdf, ax_pdf = plt.subplots()\n",
        "ax_pdf.plot(x, scipy.stats.norm.pdf(x, mean, std), color='black', label='normal distribution')\n",
        "ax_pdf.fill_between(x_fill, scipy.stats.norm.pdf(x_fill, mean, std), color='grey')\n",
        "# Get anomaly bounds\n",
        "x_fault = np.linspace(min(x), th_lower)\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "x_fault = np.linspace(th_higher, max(x))\n",
        "ax_pdf.fill_between(x_fault, scipy.stats.norm.pdf(x_fault, mean, std), color='salmon')\n",
        "ax_pdf.set_xlim(min(x), max(x))\n",
        "ax_pdf.set_ylim([0, None])\n",
        "ax_pdf.set_xlabel('Temperature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "JcHOPie_1-On",
        "outputId": "e03bb20c-5d08-4515-de89-ab6973fce5cd"
      },
      "outputs": [],
      "source": [
        "# Generate anomaly data with drift\n",
        "n_noc = 100\n",
        "n_fault_slice = 50  # 50 slices of 10 points each\n",
        "n_slice = 10\n",
        "changeRate = 1.01\n",
        "X_test = np.random.normal(mean, std, n_noc)\n",
        "\n",
        "poisson_variable = np.random.poisson(0.05, n_fault_slice);\n",
        "mu_fault = mean\n",
        "sigma_fault = std\n",
        "for i in range(0, n_fault_slice):\n",
        "    mu_fault = mu_fault * (changeRate ** poisson_variable[i])\n",
        "    sigma_fault = sigma_fault * (changeRate ** poisson_variable[i])\n",
        "    next_fault_data = np.random.normal(mu_fault, sigma_fault, n_slice)\n",
        "    X_test = np.concatenate((X_test, next_fault_data))\n",
        "\n",
        "# Time series plot\n",
        "x_fault = np.arange(1, len(X_test) + 1)\n",
        "plt.plot(x_fault, X_test)\n",
        "plt.axvline(n_noc, color='salmon', label='fault starts')\n",
        "\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_higher, th_higher], 'r--')\n",
        "plt.plot([x_fault[0], x_fault[-1]], [th_lower, th_lower], 'r--')\n",
        "\n",
        "plt.title('Anomaly generation against time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "# Histogram\n",
        "ax_pdf.hist(X_test, 20, density=True, facecolor='g', alpha=0.75, label='anomaly data')\n",
        "ax_pdf.legend(loc='best')\n",
        "fig_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4wyYVbJgT4p"
      },
      "source": [
        "Let's obtain a metric for how well does this anomaly detector worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "F-dyQgbsgToV",
        "outputId": "a3f709b8-512a-4143-e964-85a92fd0e44f"
      },
      "outputs": [],
      "source": [
        "# First we get the actual labels of our cross-validation set\n",
        "# We know that the first 100 instances are normal, while the fault starts\n",
        "#  from that point onwards\n",
        "y_true = np.concatenate((\n",
        "    np.zeros((n_noc,)),\n",
        "    np.ones((n_fault_slice * n_slice))\n",
        "))\n",
        "y_pred = (X_test > th_higher) | (X_test < th_lower)\n",
        "\n",
        "# Plot our prediction\n",
        "plt.plot(x_fault[y_pred == 0], X_test[y_pred == 0], 'b.', label='predicted as normal')\n",
        "plt.plot(x_fault[y_pred == 1], X_test[y_pred == 1], 'rx', label='predicted as anomaly')\n",
        "plt.title('Anomalies detected over time')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Reactor Temperature')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Use the accuracy_score function from skit-learn\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"The obtained accuracy is {accuracy * 100:0.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnOde5Wjut3"
      },
      "source": [
        "Note that due to our threshold, a few false positives take place. This is a common trade-off in anomaly detection:\n",
        "- **It is usually difficult to avoid both false positives and false negatives** at the same time\n",
        "- The determination of the threshold is an important task that the engineer must take into account to give more weight to one or another.\n",
        "- This depends heavily on the application: e.g. in medical applications, false positives are highly discouraged, while spam detection might be more indulgent in that sense\n",
        "\n",
        "Also note that the fault takes quite a while to manifest, from it start at $t=100$ min to the first visually noticeable drift at aroun $t=300$ min. More advanced methods might help us anticipate this anomaly but, without more variables showing more hints, it can be impossible to increase our accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NQ7pVy4jtdl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeVm9hNBrtn8"
      },
      "source": [
        "<font color = blue>Now, you will need to write code to detect the anomalies in this dataset fitting the data to a normal distribution and reporting the outliers using the overall accuracty as a metric.<\\font>\n",
        "\n",
        "To determine a threshold to distinguish normal from anomalous instances, use three times the standard deviation ($3\\sigma$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59xxCC_1rIZ3"
      },
      "outputs": [],
      "source": [
        "# ============= Start of your code ============= #\n",
        "\n",
        "\n",
        "# =============  End of your code  ============= #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JbUKIwyrYv9"
      },
      "source": [
        "<font color='blue'>Now tune this threshold using a numerical method to get a specific performance </font> (<font color='red'>not useful now since it can be demonstrated that a specific sigma leaves a specific % out of \"normality\". BUT will be useful for Autoencoders and PCA</font>) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDs5PjiGuUw1"
      },
      "source": [
        "## [Optional] repeat stacking time windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utGQzP6RuT52"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sJovavKqG0U"
      },
      "source": [
        "# Multivariate example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGsiNIJ6qMf4"
      },
      "source": [
        "Now, consider a more complex case where we have 10 variables that conform a dataset representative of the state of the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncIVzRVDwDCa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(41)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IwHfD1AqFzN"
      },
      "outputs": [],
      "source": [
        "def generate_multivariate_normal_dataset(mean, covariance, n, perc_noc, changing_rate, rng=None, shuffle=False):\n",
        "  n_noc = int (n * perc_noc)\n",
        "  n_fault = n - n_noc\n",
        "\n",
        "  x_noc = np.zeros((n_noc, 10))\n",
        "  x_fault = np.zeros((n_fault, 10))\n",
        "\n",
        "  # Generate data under normal conditions\n",
        "  x_noc = rng.multivariate_normal(mean=mean, cov=covariance, size=n_noc)\n",
        "  label_noc = np.zeros(n_noc)\n",
        "  label_noc = label_noc.reshape((len(label_noc), 1))\n",
        "  data_noc = np.concatenate((x_noc, label_noc), axis=1)\n",
        "\n",
        "  # Generate data include fault\n",
        "  poisson_generator = rng.poisson(0.2, n_fault)\n",
        "  label_poisson = np.ones(n_fault)\n",
        "  for j in range(n_fault):\n",
        "    if poisson_generator[j] != 0:\n",
        "        label_poisson[0:j+1] = 0\n",
        "        break\n",
        "\n",
        "  # Depends on whether mean or cov is changing\n",
        "  mean_fault = mean\n",
        "  cov_fault = covariance\n",
        "\n",
        "  for i in range(x_fault.shape[0]):\n",
        "    if poisson_generator[i] != 0:\n",
        "        cov_fault[0,1] *= (1+changing_rate)**(poisson_generator[i]) # make it also accumulated(to detect as soon as possible)\n",
        "        cov_fault[1,0] *= (1+changing_rate)**(poisson_generator[i])\n",
        "    instance = rng.multivariate_normal(mean=mean_fault, cov=cov_fault, size=1)\n",
        "    x_fault[i] = instance\n",
        "\n",
        "  label_poisson = label_poisson.reshape((len(label_poisson), 1))\n",
        "  data_fault = np.concatenate((x_fault, label_poisson), axis=1)\n",
        "\n",
        "  data_np = np.concatenate((data_noc, data_fault), axis=0)\n",
        "  data = pd.DataFrame(data_np, columns=['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])\n",
        "\n",
        "  if shuffle == True:\n",
        "    data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  return data, data_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CzmK7cowFfb",
        "outputId": "68e77837-f700-42c5-f3fc-bc660c5d5fda"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "n_samples = 500\n",
        "perc_noc_train = 1   # % NOC instances in the training set (100 %)\n",
        "perc_noc_test = 0.2  # % NOC instances in the test set (100 %)\n",
        "changing_rate = 0.15\n",
        "base_rng      = np.random.default_rng(42)\n",
        "\n",
        "# Generate means\n",
        "mean = base_rng.integers(low=1, high=10, size=10)\n",
        "print(mean)\n",
        "\n",
        "# Generate covariance\n",
        "covariance = np.array([[2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5],\n",
        "                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5],\n",
        "                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0]])\n",
        "print(covariance)\n",
        "\n",
        "\n",
        "data_train, data_train_np = generate_multivariate_normal_dataset(mean, covariance, n_samples, perc_noc_train, changing_rate, rng=base_rng, shuffle=False)\n",
        "data_test, data_test_np = generate_multivariate_normal_dataset(mean, covariance, n_samples, perc_noc_test, changing_rate, rng=base_rng, shuffle=False)\n",
        "X_train = data_train.drop(['label'], axis=1).to_numpy()\n",
        "y_train = data_train['label'].to_numpy()\n",
        "X_test = data_test.drop(['label'], axis=1).to_numpy()\n",
        "y_test = data_test['label'].to_numpy()\n",
        "\n",
        "# The features of the dataset correspond to the columns of X_train\n",
        "n_features = X_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4T8gFChH4iLO",
        "outputId": "12236794-0e19-4af7-ebeb-21edb2b98f33"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBLnEGXzUQm"
      },
      "source": [
        "In this case, the dataset is completly random, and disturbances are introduced in the covariance between the first and second variable, which starts suffering a drift based on a Poisson disturbance like in the single-variable example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "LS4t04AdyNMT",
        "outputId": "41d12fb9-7acc-49bb-ef3d-1ab01f2327b0"
      },
      "outputs": [],
      "source": [
        "plt.plot(data_test.iloc[:200,0])\n",
        "plt.plot(data_train.iloc[:200,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probabilistic approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We estimate the detection thresholds for each variable separately. For this, we use the probability density function of the normal distribution\n",
        "\n",
        "$f(x_i|\\mu, \\sigma^2) = \\frac{1}{{\\sqrt{2 \\pi \\sigma^2}}} e^{-\\frac{{(x - \\mu)^2}}{{2 \\sigma^2}}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "XXXXXXXXXXXXXXXX Note that the probability density function is not exactly yielding probabilites, but a measure of the density of the random distribution at a specific point. however, it serves as a way to determine how frequent a range of values are  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def probability_density(X, mean, var):\n",
        "    \"\"\"\n",
        "    Calculate the probability density of a series of instances in X given the\n",
        "    multivariate mean and covariance matrix.\n",
        "    \"\"\"\n",
        "    n = len(mean)\n",
        "    if var.ndim == 1:\n",
        "        var = np.diag(var)\n",
        "    X = X - mean\n",
        "    p = (2 * np.pi)**(- n/2) * np.linalg.det(var)**(-0.5) * \\\n",
        "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we estimate the means and covariance of our system using the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trGRozQC5nBD"
      },
      "outputs": [],
      "source": [
        "# Estimate Gaussian distribution\n",
        "mean_train = np.mean(X_train, axis=0)\n",
        "cov_train = np.cov(X_train, rowvar=False)  # By default it treats each row as a variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the threshold using the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_threshold(scores, y_true, target_acc=0.95, positive_lower=True, n_samples=1e4):\n",
        "    \"\"\" \n",
        "    Obtain threshold by searching for the closest accuracy to target_acc via \n",
        "    brute force.\n",
        "    ----------\n",
        "    Parameters\n",
        "    ----------\n",
        "    positive_lower: if True, poisitive instances are predicted when the score\n",
        "        is lower than the threshold. If False, positive instances are predicted\n",
        "        for socres higher than the threshold\n",
        "    \"\"\"\n",
        "    # Get extremes of scores\n",
        "    scores_range = np.linspace(scores.min(), scores.max(), int(n_samples))\n",
        "    # Loop them until finding the closer value to 95 % accuracy (brute force)\n",
        "    error_old = 1\n",
        "    accs = {}\n",
        "    for score_th in scores_range:\n",
        "        # Obtain predictions given the current threshold\n",
        "        if positive_lower:\n",
        "            y_pred = scores < score_th\n",
        "        else:\n",
        "            y_pred = scores > score_th\n",
        "        # Calculate accuracy and measure the error with respect target_acc\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        accs[score_th] = acc\n",
        "        error = (target_acc - acc) ** 2\n",
        "        print(f\"Obtained accuracy {acc}, error {error}\" \n",
        "              f\" with threshold = {score_th}\")\n",
        "        if error < error_old:\n",
        "            threshold = score_th \n",
        "            error_old = error\n",
        "        elif error > error_old:\n",
        "            # Avoid continuing since no better error will be obtained\n",
        "            break\n",
        "    plt.plot(pd.Series(accs), label='evolution of accuracy')\n",
        "    plt.vlines(threshold, plt.gca().get_ylim()[0], 1, color='red', label='final threshold')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    return threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get probabilities and threshold\n",
        "probs = probability_density(X_train, mean_train, cov_train)\n",
        "threshold = get_threshold(probs, y_train, positive_lower=True, n_samples=1e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check we obtain the desired accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_pred = probability_density(X_train, mean_train, cov_train) < threshold\n",
        "print(f'We should have {int(len(X_train) * 0.05)} false positives.')\n",
        "print(f'We have {sum(y_train_pred)} false positives')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our threshold, we can evaluate the test set. Remember we use the fitted mean and variance from the training set, we do not obtain these from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_test = probability_density(X_test, mean_train, cov_train) \n",
        "y_test_pred = p_test < threshold\n",
        "acc_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'The obtained accuracy is {acc_test *100} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8c-dfj0BnU"
      },
      "source": [
        "## Proximity-based approach: Mahalanobis distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute centroid of the training set, corresponding to the estimated mean of each feature\n",
        "means = X_train.mean(axis=0)\n",
        "print(means)\n",
        "cov = np.cov(X_train, rowvar=False)\n",
        "inv_cov = np.linalg.inv(cov)\n",
        "print(inv_cov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "def compute_distances(X, means, inv_cov):\n",
        "    \"\"\"\n",
        "    Compute the Mahalanobis distance between each normal data point and the \n",
        "    cluster centers.\n",
        "    \"\"\"\n",
        "    distances_train = []\n",
        "    for i in range(X.shape[0]):\n",
        "        distances_train.append(mahalanobis(X[i, :], means, inv_cov))\n",
        "    return np.array(distances_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distances_train = compute_distances(X_train, means, inv_cov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(distances_train, 20, density=True, facecolor='g', alpha=0.75, label='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "th_mahalanobis = get_threshold(distances_train, y_train, positive_lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_train = distances_train > th_mahalanobis\n",
        "print(f'We should have {int(len(X_train) * 0.05)} false positives.')\n",
        "print(f'We have {sum(y_pred_train)} false positives')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "distances_test = compute_distances(X_test, means, inv_cov)\n",
        "y_pred_test = distances_test > th_mahalanobis\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "print(f'The obtained accuracy is {acc_test *100} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural networks: autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks are usually trained in a supervised manner. To be able to use them in a semisupervised problem, we cannot use the typical feedforward neural network. Fo this, the self-associative neural networks, also called autoencoders, come to help. Their name is self-explanatory: the goal of autoencorders is to try to reproduce the input in the output of the neural network. This may look like a trivial task but the trick here is that there is a bottleneck in the architecture of the network that causes a loss of information.\n",
        "\n",
        "The training phase tries to make the reconstruction with the lowest possible loss despite the bottleneck. This way it is able to find the most important features and discard noise that is useless to reconstruct the input. Its usefulness in anomaly detection comes with that, if trained with normal operation data and learns to identify and reconstruct it correctly, when a abnormal event occurs, the reconstruction will show an important bias compared with nomal operation data.\n",
        "\n",
        "Therefore, they do not use labels to train the network. However, we need to know that all the instances fed during training belong to the same class (usually the *normal operation* class), therefore the *semisupervised* surname of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will work with the mnist dataset. For this, we will try to identify as an anomaly every number that is not a \"5\"\n",
        "\n",
        "![title](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will load the dataset from the scikit-learn builtin datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_mnist(X, y, n=3, title=''):\n",
        "    \"\"\" Distribute plots in rows of 4. \"\"\"\n",
        "    from math import ceil\n",
        "    nrows = ceil(n / 4)\n",
        "    ncols = 4\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "    i = 0\n",
        "    for image, label in zip(X, y):\n",
        "        #np.unravel_index(i, axes.shape)\n",
        "        ax = axes.flatten()[i]\n",
        "        ax.set_axis_off()\n",
        "        image = image.reshape(8, 8)\n",
        "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "        if not label:\n",
        "            ax.set_title(f\"Label: {bool(label)}\", fontweight=\"bold\")\n",
        "        else:\n",
        "            ax.set_title(f\"Label label: {bool(label)}\")\n",
        "        i += 1\n",
        "        if i >= n:\n",
        "            break\n",
        "    fig.suptitle(title)\n",
        "\n",
        "\n",
        "def get_mnist_5(split=False):\n",
        "    # Load data\n",
        "    digits = datasets.load_digits()\n",
        "\n",
        "    # Get input and output data\n",
        "    X = digits.data\n",
        "    labels = digits.target\n",
        "    y = ~ (labels == 5)\n",
        "\n",
        "    if split:\n",
        "        # Split data into 50% train and 50% test subsets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, shuffle=True\n",
        "        )\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    else:\n",
        "        return X, y\n",
        "    \n",
        "    \n",
        "X_train, X_test, y_train, y_test = get_mnist_5(split=True)\n",
        "plot_mnist(X_train, y_train, n=12)\n",
        "print(\"Five's are represented as False, while the rest is considered True\")\n",
        "\n",
        "# Keep only 5's for the training set\n",
        "X_train = X_train[y_train == False]\n",
        "y_train = y_train[y_train == False]\n",
        "plot_mnist(X_train, y_train, n=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each instance of the training set consists of 64 elements that represent the 8-by-8 greyscale pixel image of a handwritten number. Each pixel value ranges from 0 to 16, from white to black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train[1, :], y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before working with the data, we need to standardize the pixel intensisty values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_norm = X_train / 16\n",
        "X_test_norm = X_test / 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have our dataset, we can set up our autoencoder neural network.\n",
        "\n",
        "First, set up the architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shape of input and latent variable\n",
        "n_input = X_train.shape[1]\n",
        "\n",
        "# Encoder structure\n",
        "n_encoder1 = 10\n",
        "# n_encoder2 = 10\n",
        "n_latent = 2\n",
        "\n",
        "# Decoder structure\n",
        "# n_decoder2 = 10\n",
        "n_decoder1 = 10\n",
        "\n",
        "hidden_layer_sizes = (n_encoder1, n_latent, n_decoder1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MLPRegressor(\n",
        "    hidden_layer_sizes=hidden_layer_sizes, \n",
        "    activation='tanh', \n",
        "    solver='adam', \n",
        "    batch_size='auto', \n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=1000,\n",
        "    random_state=None,\n",
        "    tol=0.0000001,\n",
        "    verbose=True,\n",
        "    alpha=0.0001\n",
        ")\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can fit the model. Bear in mind that we want the model to try to obtain a target output as similar as possible to the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train_norm, X_train_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(model.loss_curve_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check how reconstructions go for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_inout_mnist(image_in, image_out):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "    axes[0].imshow(image_in, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[0].set_title(\"Input\")\n",
        "    axes[1].imshow(image_out, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    axes[1].set_title(\"Output\")\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 5\n",
        "input_image = X_train_norm[0, :].reshape((8,  8))\n",
        "# Perform reconstruction through autoencoder\n",
        "reconstructed_image = model.predict(X_train_norm[0:0 + 1, :])\n",
        "reconstructed_image = reconstructed_image.reshape((8, 8))\n",
        "compare_inout_mnist(input_image, reconstructed_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_norm[:, :].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot other numbers in the test set\n",
        "reconstructed_images = []\n",
        "for i in range(1, 10):\n",
        "    input_image = X_test_norm[i, :].reshape((8,  8))\n",
        "    # Perform reconstruction through autoencoder\n",
        "    reconstructed_image = model.predict(X_test_norm[i:i + 1, :])\n",
        "    print(reconstructed_image.max(), reconstructed_image.min())\n",
        "    reconstructed_image = reconstructed_image.reshape((8, 8))\n",
        "    compare_inout_mnist(input_image, reconstructed_image)\n",
        "    reconstructed_images.append(reconstructed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Now explain how to measure the reconstruction error**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
