# -*- coding: utf-8 -*-
"""Multivariate data_PCA_0409.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15qhzIXE4vI2JZ-WPPsvkS-_jxvUl6XM5
"""

import numpy as np
import numpy.random as rnd
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import scipy as sp
from sklearn.decomposition import PCA

def generate_multivariate_normal_dataset(mean, covariance, n, perc_noc, changing_rate, rng=None, shuffle=False):
  n_noc = int (n * perc_noc)
  n_fault = n - n_noc

  x_noc = np.zeros((n_noc, 10))
  x_fault = np.zeros((n_fault, 10))

  # Generate data under normal conditions
  x_noc = rng.multivariate_normal(mean=mean, cov=covariance, size=n_noc)
  label_noc = np.zeros(n_noc)
  label_noc = label_noc.reshape((len(label_noc), 1))
  data_noc = np.concatenate((x_noc, label_noc), axis=1)

  # Generate data include fault
  poisson_generator = rng.poisson(0.2, n_fault)
  label_poisson = np.ones(n_fault)
  for j in range(n_fault):
    if poisson_generator[j] != 0:
        label_poisson[0:j+1] = 0
        break

  # Depends on whether mean or cov is changing
  mean_fault = mean
  cov_fault = covariance

  for i in range(x_fault.shape[0]):
    if poisson_generator[i] != 0:
        cov_fault[0,1] *= (1+changing_rate)**(poisson_generator[i]) # make it also accumulated(to detect as soon as possible)
        cov_fault[1,0] *= (1+changing_rate)**(poisson_generator[i])
    instance = rng.multivariate_normal(mean=mean_fault, cov=cov_fault, size=1)
    x_fault[i] = instance

  label_poisson = label_poisson.reshape((len(label_poisson), 1))
  data_fault = np.concatenate((x_fault, label_poisson), axis=1)

  data_np = np.concatenate((data_noc, data_fault), axis=0)
  data = pd.DataFrame(data_np, columns=['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

  if shuffle == True:
    data.sample(frac=1).reset_index(drop=True)

  return data, data_np

n_sample = 500
perc_noc_train = 1
perc_noc_test = 0.8
changing_rate = 0.15
base_rng      = np.random.default_rng(42)

# Generate mean
mean = base_rng.integers(low=1, high=10, size=10)
print(mean)

# Generate covariance
covariance = np.array([[2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0]])
print(covariance)



data_train, data_train_np = generate_multivariate_normal_dataset(mean, covariance, n_sample, perc_noc_train, changing_rate, rng=base_rng, shuffle=False)
data_test, data_test_np = generate_multivariate_normal_dataset(mean, covariance, n_sample, perc_noc_test, changing_rate, rng=base_rng, shuffle=False)

data_test

from sklearn.preprocessing import StandardScaler

# Normalize the data
scaler = StandardScaler()

Data_train_nolabel = scaler.fit_transform(data_train_np[:,0:10])
label_train = data_train_np[:,10]
label_train = label_train.reshape((len(label_train), 1))
Data_train = np.concatenate((Data_train_nolabel, label_train), axis=1)
Data_train_df = pd.DataFrame(Data_train, columns = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

Data_test_nolabel = scaler.transform(data_test_np[:,0:10])
label_test = data_test_np[:,10]
label_test = label_test.reshape((len(label_test), 1))
Data_test = np.concatenate((Data_test_nolabel, label_test), axis=1)
Data_test_df = pd.DataFrame(Data_test, columns = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

# Print the mean and standard deviation of the normalized data
print(np.mean(Data_train_nolabel, axis=0))  # Should be close to zero
print(np.std(Data_train_nolabel, axis=0))  # Should be close to one

print(np.mean(Data_test_nolabel, axis=0))  # Should be close to zero
print(np.std(Data_test_nolabel, axis=0))  # Should be close to one

x_axis = np.linspace(0, n_sample-1, n_sample)

plt.plot(x_axis, Data_train[:,0:2])
plt.plot(x_axis, Data_train[:,2], color='paleturquoise')
plt.title("Data_train")
plt.legend(labels=["x1","x2","x3"])
plt.show()

plt.plot(x_axis, Data_test[:,0:2])
plt.plot(x_axis, Data_test[:,2], color='paleturquoise')
plt.title("Data_test")
plt.legend(labels=["x1","x2","x3"])
plt.show()

pca_10 = PCA(n_components=0.9, random_state=42)
pca_10.fit(Data_test_nolabel)
Data_test_pca_10 = pca_10.transform(Data_test_nolabel)
np.cumsum(pca_10.explained_variance_ratio_*100)

plt.plot(np.cumsum(pca_10.explained_variance_ratio_*100))

pca = PCA(n_components=8, random_state=42)
pca_df = pd.DataFrame(pca.fit_transform(Data_test_nolabel), index=Data_test_df.index)

# restore PCA data
pca_data = pd.DataFrame(pca.inverse_transform(pca_df), index=pca_df.index)

pca.explained_variance_ratio_

# Fit the PCA model
n_components = 0.9 # Number of principal components to keep
pca = PCA(n_components=n_components)
pca.fit(Data_train_nolabel)

# Transform the data into the reduced feature space
data_train_reduced = pca.transform(Data_train_nolabel)
# Reconstruct the data from the reduced feature space
data_train_reconstructed = pca.inverse_transform(data_train_reduced)
# Compute the reconstruction error
reconstruction_error_train = np.linalg.norm(Data_train_nolabel - data_train_reconstructed, axis=1)

threshold = 1.87
anomalies_tuning = np.where(reconstruction_error_train > threshold)[0]

y_pred_np_train = np.zeros(n_sample, dtype=bool)
y_pred_np_train[anomalies_tuning] = True
y_pred_train = pd.DataFrame(y_pred_np_train)
y_true_train = Data_train_df['label']

from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(y_true_train, y_pred_train)
print(f'test accuracy for threshold tuning = {accuracy_train}')

# Do the same thing for data test
# pca.fit(Data_test_nolabel)
data_test_reduced = pca.transform(Data_test_nolabel)
data_test_reconstructed = pca.inverse_transform(data_test_reduced)
reconstruction_error_test = np.linalg.norm(Data_test_nolabel - data_test_reconstructed, axis=1)

anomalies = np.where(reconstruction_error_test > threshold)[0]
anomalies

# visualize the result
plt.plot(x_axis, Data_test[:,0:2])
plt.plot(x_axis, Data_test[:,2], color='paleturquoise')
for line in anomalies:
    plt.axvline(x=line, color='r', alpha=0.1)
plt.title("Data_test_PCA")
plt.legend(labels=["x1","x2","x3","anomalies"])
plt.show()

# Test the performance
# Store predictions as '1' and '0'; or 'True' and 'False'

y_pred_np = np.zeros(n_sample, dtype=bool)
y_pred_np[anomalies] = True
y_pred = pd.DataFrame(y_pred_np)

y_true = Data_test_df['label']

y_pred,y_true

from sklearn.metrics import accuracy_score, recall_score, precision_score, \
                            f1_score, confusion_matrix

accuracy_test = accuracy_score(y_true, y_pred)
precision_test = precision_score(y_true, y_pred)
recall_test = recall_score(y_true, y_pred)
f1_test = f1_score(y_true, y_pred)

print(f'test accuracy = {accuracy_test}')
print(f'test precision = {precision_test}')
print(f'test recall = {recall_test}')
print(f'test F1 score = {f1_test}')

from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Define the labels for the matrix
x_labels = ['Negative','Positive']
y_labels = ['False','True']

# Create a heatmap visualization of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=x_labels, yticklabels=y_labels)

# Add labels to the plot
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix_PCA')

# Show the plot
plt.show()

y_true.describe()