# -*- coding: utf-8 -*-
"""Multivariate data_Clustering_0409.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14xynzvLRLM6F-_NDejl5yyTBB5I9un45
"""

import numpy as np
import numpy.random as rnd
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import scipy as sp

def generate_multivariate_normal_dataset(mean, covariance, n, perc_noc, changing_rate, rng=None, shuffle=False):
  n_noc = int (n * perc_noc)
  n_fault = n - n_noc

  x_noc = np.zeros((n_noc, 10))
  x_fault = np.zeros((n_fault, 10))

  # Generate data under normal conditions
  x_noc = rng.multivariate_normal(mean=mean, cov=covariance, size=n_noc)
  label_noc = np.zeros(n_noc)
  label_noc = label_noc.reshape((len(label_noc), 1))
  data_noc = np.concatenate((x_noc, label_noc), axis=1)

  # Generate data include fault
  poisson_generator = rng.poisson(0.2, n_fault)
  label_poisson = np.ones(n_fault)
  for j in range(n_fault):
    if poisson_generator[j] != 0:
        label_poisson[0:j+1] = 0
        break

  # Depends on whether mean or cov is changing
  mean_fault = mean
  cov_fault = covariance

  for i in range(x_fault.shape[0]):
    if poisson_generator[i] != 0:
        cov_fault[0,1] *= (1+changing_rate)**(poisson_generator[i]) # make it also accumulated(to detect as soon as possible)
        cov_fault[1,0] *= (1+changing_rate)**(poisson_generator[i])
    instance = rng.multivariate_normal(mean=mean_fault, cov=cov_fault, size=1)
    x_fault[i] = instance

  label_poisson = label_poisson.reshape((len(label_poisson), 1))
  data_fault = np.concatenate((x_fault, label_poisson), axis=1)

  data_np = np.concatenate((data_noc, data_fault), axis=0)
  data = pd.DataFrame(data_np, columns=['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

  if shuffle == True:
    data.sample(frac=1).reset_index(drop=True)

  return data, data_np

n_sample = 500
perc_noc_train = 1
perc_noc_test = 0.8
changing_rate = 0.15
base_rng      = np.random.default_rng(42)

# Generate mean
mean = base_rng.integers(low=1, high=10, size=10)
print(mean)

# Generate covariance
covariance = np.array([[2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5, 0.5],
                      [0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0, -0.5],
                      [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 0.5, -0.5, 2.0]])
print(covariance)



data_train, data_train_np = generate_multivariate_normal_dataset(mean, covariance, n_sample, perc_noc_train, changing_rate, rng=base_rng, shuffle=False)
data_test, data_test_np = generate_multivariate_normal_dataset(mean, covariance, n_sample, perc_noc_test, changing_rate, rng=base_rng, shuffle=False)

data_test_np

from sklearn.preprocessing import StandardScaler

# Normalize the data
scaler = StandardScaler()

Data_train_nolabel = scaler.fit_transform(data_train_np[:,0:10])
label_train = data_train_np[:,10]
label_train = label_train.reshape((len(label_train), 1))
Data_train = np.concatenate((Data_train_nolabel, label_train), axis=1)
Data_train_df = pd.DataFrame(Data_train, columns = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

Data_test_nolabel = scaler.transform(data_test_np[:,0:10])
label_test = data_test_np[:,10]
label_test = label_test.reshape((len(label_test), 1))
Data_test = np.concatenate((Data_test_nolabel, label_test), axis=1)
Data_test_df = pd.DataFrame(Data_test, columns = ['x1', 'x2', 'x3', 'x4','x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'label'])

# Print the mean and standard deviation of the normalized data
print(np.mean(Data_train_nolabel, axis=0))  # Should be close to zero
print(np.std(Data_train_nolabel, axis=0))  # Should be close to one

print(np.mean(Data_test_nolabel, axis=0))  # Should be close to zero
print(np.std(Data_test_nolabel, axis=0))  # Should be close to one

x_axis = np.linspace(0, n_sample-1, n_sample)

plt.plot(x_axis, Data_train[:,0:2])
plt.plot(x_axis, Data_train[:,2], color='paleturquoise')
plt.title("Data_train")
plt.legend(labels=["x1","x2","x3"])
plt.show()

plt.plot(x_axis, Data_test[:,0:2])
plt.plot(x_axis, Data_test[:,2], color='paleturquoise')
plt.title("Data_test")
plt.legend(labels=["x1","x2","x3"])
plt.show()

n = 10 # number of data each sample
gap = 1 # gap between each sample

# Create an empty DataFrame with column names
data_train_mean = pd.DataFrame(columns=["mu1","mu2","mu3","mu4","mu5","mu6","mu7","mu8","mu9","mu10"])
data_train_std = pd.DataFrame(columns=["sigma1","sigma2","sigma3","sigma4","sigma5","sigma6","sigma7","sigma8","sigma9","sigma10"])

# Loop over the data and add rows to the DataFrame
for i in range(0, len(Data_train)-n+1, gap):
    # Calculate the mean for each column of the current sample
    sample_mean = [Data_train[i:i+n,k].mean() for k in range(10)]
    # Add the new row of means to the DataFrame
    data_train_mean.loc[i] = sample_mean
# Reset the index to a simple integer index
data_train_mean = data_train_mean.reset_index(drop=True)

# Do the same thing for std
for i in range(0, len(Data_train)-n+1, gap):
    sample_std = [Data_train[i:i+n,k].std() for k in range(10)]
    data_train_std.loc[i] = sample_std
data_train_std = data_train_std.reset_index(drop=True)

Data_train_mean_std = pd.concat([data_train_mean, data_train_std], axis=1)
Data_train_mean_std

# Do the same for test data
data_test_mean = pd.DataFrame(columns=["mu1","mu2","mu3","mu4","mu5","mu6","mu7","mu8","mu9","mu10"])
data_test_std = pd.DataFrame(columns=["sigma1","sigma2","sigma3","sigma4","sigma5","sigma6","sigma7","sigma8","sigma9","sigma10"])

for i in range(0, len(Data_test)-n+1, gap):
    sample_mean = [Data_test[i:i+n,k].mean() for k in range(10)]
    data_test_mean.loc[i] = sample_mean
data_test_mean = data_test_mean.reset_index(drop=True)

for i in range(0, len(Data_test)-n+1, gap):
    sample_std = [Data_test[i:i+n,k].std() for k in range(10)]
    data_test_std.loc[i] = sample_std
data_test_std = data_test_std.reset_index(drop=True)

Data_test_mean_std = pd.concat([data_test_mean, data_test_std], axis=1)
Data_test_mean_std

from sklearn.cluster import KMeans

# choose the number of clusters (k)
k = 1

# initialize the KMeans object with k clusters
kmeans = KMeans(n_clusters=k, random_state=0)

# fit the KMeans model to the data
kmeans.fit(Data_train_mean_std)

# get the centroid locations for each cluster
centroids = kmeans.cluster_centers_

centroids

from scipy.spatial.distance import mahalanobis

# Compute the Mahalanobis distance between each normal data point and the cluster centers
distances_train = []
for i in range(Data_train_mean_std.shape[0]):
    points_cov = [centroids[0], Data_train_mean_std.iloc[i]]
    cov = np.cov(points_cov, rowvar=False) # calculate the covariance matrix
    distances_train.append(mahalanobis(Data_train_mean_std.iloc[i], centroids[0], cov))

# Identify anomalies with a Mahalanobis distance above a threshold
threshold = 2.62
anomalies_tuning = np.where(np.array(distances_train) > threshold)[0]
anomalies_tuning = anomalies_tuning+n-1

y_pred_np_train = np.zeros(n_sample, dtype=bool)
y_pred_np_train[anomalies_tuning] = True
y_pred_train = pd.DataFrame(y_pred_np_train)
y_true_train = Data_train_df['label']

from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(y_true_train, y_pred_train)
print(f'test accuracy for threshold tuning = {accuracy_train}')

# Data test
# Compute the Mahalanobis distance between each data point (with anomaly) and the cluster centers
distances_test = []
for i in range(Data_test_mean_std.shape[0]):
    points_cov = [centroids[0], Data_test_mean_std.iloc[i]]
    cov = np.cov(points_cov, rowvar=False) # calculate the covariance matrix
    distances_test.append(mahalanobis(Data_test_mean_std.iloc[i], centroids[0], cov))

anomalies_sample = np.where(np.array(distances_test) > threshold)[0]
anomalies = anomalies_sample+n-1

# visualize the result
plt.plot(x_axis, Data_test[:,0:2])
plt.plot(x_axis, Data_test[:,2], color='paleturquoise')
for line in anomalies:
    plt.axvline(x=line, color='r', alpha=0.1)
plt.title("Data_test_Clustering")
plt.legend(labels=["x1","x2","x3","anomalies"])
plt.show()

# Test the performance
# Store predictions as '1' and '0'; or 'True' and 'False'

y_pred_np = np.zeros(n_sample, dtype=bool)
y_pred_np[anomalies] = True
y_pred = pd.DataFrame(y_pred_np)

y_true = Data_test_df['label']

y_pred,y_true

from sklearn.metrics import accuracy_score, recall_score, precision_score, \
                            f1_score, confusion_matrix

accuracy_test = accuracy_score(y_true, y_pred)
precision_test = precision_score(y_true, y_pred)
recall_test = recall_score(y_true, y_pred)
f1_test = f1_score(y_true, y_pred)

print(f'test accuracy = {accuracy_test}')
print(f'test precision = {precision_test}')
print(f'test recall = {recall_test}')
print(f'test F1 score = {f1_test}')

from sklearn.metrics import confusion_matrix

# Compute the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Define the labels for the matrix
x_labels = ['Negative','Positive']
y_labels = ['False','True']

# Create a heatmap visualization of the confusion matrix
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=x_labels, yticklabels=y_labels)

# Add labels to the plot
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix_Clustering')

# Show the plot
plt.show()

y_true.describe()